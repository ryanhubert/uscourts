#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Ryan HÃ¼bert
# Department of Political Science
# University of California, Davis

"""
dockets.ParseDocketEntries v1.0
Extracts entries from HTML formatted docket sheet from CM/ECF and returns a dictionary
with data and text from each entry.
"""

import re
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, date

def TerminationWindow(html_string, windows=[1,0]):
    """
    Takes HTML of docket sheet and identifies a case filed and terminated date
    and builds a window of dates to parse in entries.
    :param html_string: str HTML of docket sheet
    :param windows: list with days before and after termination date that we should parse
        - NB: examination of dockets reveals optimal default: one day prior and zero days post
    :return: list with filing and termination dates
    """

    chunk = html_string[html_string.find('</h3'):]
    chunk = chunk[chunk.find('<table'):]
    chunk = chunk[:chunk.find('<u>')]
    chunk = chunk.replace('s:', ':')
    chunk = chunk.replace('\t', '\n')
    chunk = chunk.replace("<br>", "\n")

    soup = BeautifulSoup(chunk, 'lxml')

    tables = [' '.join(x.split()).strip() for x in soup.text.split('\n') if ":" in x]
    tables = [x.split(': ') for x in tables if len(x.split(': ')) > 1]
    tables = [['_'.join(x[0].lower().split()).strip(), x[1].strip()] for x in tables]
    tables = [[x[0] if x[0][-1] != 's' else x[0][:-1], x[1].strip()] for x in tables]

    termd = [x[1] for x in tables if x[0] in ['date_terminated']]
    termd = termd[0] if len(termd) > 0 else ''
    if '/' in termd:
        termd = datetime.strptime(termd, '%m/%d/%Y').date()
    else:
        termd = None

    if termd is None:
        return None

    window = [termd]
    for i in range(windows[0]):
        if termd.weekday() not in [0, 4]:
            window.append(termd - timedelta(1+i)) #1
        elif termd.weekday() == 0:
            window.append(termd - timedelta(3+i)) #3
        elif termd.weekday() == 4:
            window.append(termd - timedelta(1+i)) #1
    for i in range(windows[1]):
        if termd.weekday() not in [0, 4]:
            window.append(termd + timedelta(1+i)) #1
        elif termd.weekday() == 0:
            window.append(termd + timedelta(1+i)) #1
        elif termd.weekday() == 4:
            window.append(termd + timedelta(2+i)) #3

    return [min(window), max(window)]

# Compile some regex for faster parsing

re_link1 = re.compile('href *\= *[\"\']([^\"\']+)[\"\']')
re_link2 = re.compile('</?td[^>]*>')
re_link3 = re.compile('<a href *\= *[\"\']([^\"\']+)[\"\'][^>]*>([^<]*)</a>')

def ExtractEntries(file_path_or_bs4obj, date_range = []):
    """
    ExtractEntries v1.0
    Take a beautiful soup object and return a dictionary of a docket's entries.

    :param file_path_or_bs4obj: str specifying an HTML formatted docket sheet from ECF
            OR a beautiful soup object (for efficiency)
    :param date_range: list start/end dates generated by TerminationWindow()
    :return: dict of docket entries
    """

    if date_range is None: # No termination date!
        return {}

    if type(file_path_or_bs4obj) is str:
        html_text = open(file_path_or_bs4obj, 'r').read()
        soup = BeautifulSoup(html_text, 'lxml')
    else:
        soup = file_path_or_bs4obj

    table = [x for x in soup.find_all('table') if "Docket Text" in x.text]
    table = table[0] if table != [] else None

    if table is not None:
        df = {}
        linksout = {}
        for r in range(1, len(table.find_all('tr'))):
            row = [x for x in table.find_all('tr')[r].find_all("td")]
            entry_date = datetime.strptime(row[0].text, '%m/%d/%Y').date()
            if len(date_range) == 2 and not (date_range[0] <= entry_date <= date_range[1]):
                continue
            df[r] = {}
            df[r]['date'] = entry_date
            df[r]['doc_num'] = row[1].text.strip() if re.search('[^ ]', row[1].text) else None
            df[r]['doc_link'] = re.findall(re_link1, str(row[1]))
            df[r]['doc_link'] = df[r]['doc_link'][0] if df[r]['doc_link'] != [] else None
            if df[r]['doc_link'] is not None:
                linksout[df[r]['doc_link']] = r
            df[r]['entry_text'] = row[2].text.strip()

            entry_links = str(row[2])
            entry_links = re.sub(re_link2, '', entry_links)
            entry_links = re.sub(re_link3, '<\\1>{\\2}', entry_links)
            for l in linksout:
                entry_links = re.sub("<" + l + ">\{([^\}]*)\}", '<' + str(linksout[l]) + '>{\\1}', entry_links)
            df[r]['linked_entries'] = sorted([int(x) for x in re.findall('<(\d+)>', entry_links)])
    else:
        df = {}

    return df
